{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# download the conceptnet complete data\n",
    "#!wget https://s3.amazonaws.com/conceptnet/downloads/2019/edges/conceptnet-assertions-5.7.0.csv.gz"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0sGOaFh7ZGT",
    "outputId": "b64c233b-fb16-44f8-d293-f6bed5dcbf26"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2023-10-09 16:11:55--  https://s3.amazonaws.com/conceptnet/downloads/2019/edges/conceptnet-assertions-5.7.0.csv.gz\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.106.230, 16.182.37.200, 52.216.171.53, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.106.230|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 497963447 (475M) [application/x-gzip]\n",
      "Saving to: ‘conceptnet-assertions-5.7.0.csv.gz’\n",
      "\n",
      "conceptnet-assertio 100%[===================>] 474.89M  50.5MB/s    in 9.8s    \n",
      "\n",
      "2023-10-09 16:12:05 (48.5 MB/s) - ‘conceptnet-assertions-5.7.0.csv.gz’ saved [497963447/497963447]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# unzipping the data\n",
    "!gunzip conceptnet-assertions-5.7.0.csv.gz"
   ],
   "metadata": {
    "id": "AhvQtqV57eM8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import csv\n",
    "s = set()\n",
    "c = 0\n",
    "ans = []\n",
    "with open('conceptnet-assertions-5.7.0.csv', 'r') as f:\n",
    "  reader = csv.reader(f)\n",
    "  for row in reader:\n",
    "    if(row[0].startswith('/a/[/r/IsA/')) and row[1].startswith('/c/en/'):\n",
    "      ans.append(row)\n"
   ],
   "metadata": {
    "id": "dN6fG3rn78xG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(ans)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cfwv7LnYAAW9",
    "outputId": "b5e1d262-415f-4851-83e5-3ec1077708eb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "230813"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(ans[:100])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mkim9k6dCnIP",
    "outputId": "60aba5f0-3898-4fa1-f836-d3cc7b6300ee"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# collecting all the words from conceptnet\n",
    "s = set()\n",
    "c = 0\n",
    "for i in ans:\n",
    "  word = i[1].split('/')\n",
    "  if(word[3].isalpha()):\n",
    "    s.add(word[3])"
   ],
   "metadata": {
    "id": "r1gTQLvvIu-V"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# data.json is the data collected from the Webster dictionary using the script \"\"  from\n",
    "# https://github.com/matthewreagan/WebstersEnglishDictionary/blob/master/WebstersEnglishDictionary.txt\n",
    "import json\n",
    "words_set = set()\n",
    "f = open(\"data.json\")\n",
    "dictionary_words = json.loads(f.read())\n",
    "final_words = dictionary_words\n",
    "for i in dictionary_words:\n",
    "  words_set.add(i['word'])"
   ],
   "metadata": {
    "id": "JMp8rNUz4opK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "for i in final_words:\n",
    "  i['source'] = dict()\n",
    "  i['source']['word'] = 'websterdictionary'\n",
    "conceptnet_added_words = 0\n",
    "# iterating on the conceptnet words and adding them\n",
    "# we eliminated words that contain _ or numbers or any other special chars\n",
    "for i in conceptnet_words:\n",
    "  word = i[1].split('/')\n",
    "  if(word[3].isalpha()):\n",
    "    # words_set is set checking a word before adding them to remove duplicates\n",
    "    if word[3] not in words_set:\n",
    "      conceptnet_added_words+=1\n",
    "      d = dict()\n",
    "      d['word'] = word[3]\n",
    "      d['source'] = dict()\n",
    "      d['source']['word'] = 'conceptnet'\n",
    "      final_words.append(d)\n",
    "      words_set.add(word[3])\n",
    "print(conceptnet_added_words)\n",
    "print(len(final_words))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjYsbtNkQCkQ",
    "outputId": "5b06237b-6108-4601-d67b-8f8fc4be4139"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33288\n",
      "129173\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# adding all words from wordnet\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wordnet_added_words = 0\n",
    "for synset in wordnet.all_synsets():\n",
    "  for lemma in synset.lemmas():\n",
    "    if lemma.name() not in words_set and lemma.name().isalpha():\n",
    "        words_set.add(lemma.name())\n",
    "        d = dict()\n",
    "        d['word'] = lemma.name()\n",
    "        d['source'] = dict()\n",
    "        d['source']['word'] = 'wordnet'\n",
    "        final_words.append(d)\n",
    "        wordnet_added_words +=1\n",
    "print(wordnet_added_words)\n",
    "print(len(final_words))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FoYGj9MaQzOU",
    "outputId": "34563cd0-87e4-4692-d7bc-48cb5a3119af"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30116\n",
      "159289\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(words_set))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83K56N2248QO",
    "outputId": "5e44f4e7-761b-4bee-a641-56ebeb517ce5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "159289\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# adding the nltk data it has a lot of nouns too\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "arpabet = nltk.corpus.cmudict.dict()\n",
    "cmudict_added_words = 0\n",
    "for i in arpabet:\n",
    "  if i not in words_set and i.isalpha():\n",
    "    d = dict()\n",
    "    d['word'] = i\n",
    "    d['source'] = {'word' : 'cmudict'}\n",
    "    words_set.add(i)\n",
    "    final_words.append(d)\n",
    "    cmudict_added_words+=1\n",
    "\n",
    "print(cmudict_added_words)\n",
    "print(len(final_words))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tnENtG5x9Mof",
    "outputId": "da491797-1004-4eec-de1a-1f96832e60d7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "77231\n",
      "236520\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i in final_words:\n",
    "  if i['source']['word'] == 'cmudict':\n",
    "    print(i['word'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wGCexio9OGY",
    "outputId": "8e4e7d2b-0413-4564-b3f0-7572122e8a4e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# saving the data to final_data.json file\n",
    "final_final_words.sort(key = lambda x : x['word'])\n",
    "with open(\"final_data.json\", 'w') as fd:\n",
    "    json.dump(final_final_words, fd, indent= 2)\n",
    "f.close()"
   ],
   "metadata": {
    "id": "LyRO1Doh_m86"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "final_final_words = []\n",
    "s = set()\n",
    "for i in final_words:\n",
    "  if i['word'].lower() not in s:\n",
    "    i['word'] = i['word'].lower()\n",
    "    final_final_words.append(i)\n",
    "    s.add(i['word'].lower())"
   ],
   "metadata": {
    "id": "D229Y-VlCIRd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(final_final_words))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0xAjcpaDSsN",
    "outputId": "c1c8f9cd-e299-4f7e-baa5-1992c0c88dcb"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "224343\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(final_words))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH_CDLRUDWyC",
    "outputId": "4266a28a-5be5-4117-9c9f-de2465e56d59"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "236520\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "X4OhTcnhDYuV"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
